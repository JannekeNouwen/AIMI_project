Hello, this is a ULS job training
The starting time is Thu May 30 22:00:40 CEST 2024
This version is trained on nnUNet_preprocessed
Training on 2d
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': [256, 256], 'median_image_size_in_voxels': [255.0, 255.0], 'spacing': [0.7578125, 0.7578125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [6, 6], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_seg': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset501_RadboudumcBone', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.800000011920929, 0.7578125, 0.7578125], 'original_median_shape_after_transp': [128, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlannerNoResampling', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2782.0, 'mean': 1175.083251953125, 'median': 1230.0, 'min': -935.0, 'percentile_00_5': 33.0, 'percentile_99_5': 2269.0, 'std': 513.393310546875}}} 

2024-05-30 22:02:47.501081: unpacking dataset...
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:121: UserWarning: CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
2024-05-30 22:03:03.269254: unpacking done...
2024-05-30 22:03:03.277659: do_dummy_2d_data_aug: False
2024-05-30 22:03:03.304713: Using splits from existing split file: /scratch-local/ljulius.6443845/nnUNet_preprocessed/Dataset501_RadboudumcBone/splits_final.json
2024-05-30 22:03:03.305676: The split file contains 5 splits.
2024-05-30 22:03:03.305775: Desired fold for training: 0
2024-05-30 22:03:03.305842: This split has 557 training and 140 validation cases.
2024-05-30 22:03:03.725327: Unable to plot network architecture:
2024-05-30 22:03:03.725468: No module named 'IPython'
2024-05-30 22:03:03.762016: 
2024-05-30 22:03:03.762151: Epoch 0
2024-05-30 22:03:03.762343: Current learning rate: 0.005
/home/ljulius/miniconda3/envs/uls/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
using pin_memory on device 0
using pin_memory on device 0
2024-05-30 22:06:08.017826: train_loss 0.0635
2024-05-30 22:06:08.018153: val_loss -0.04
2024-05-30 22:06:08.018264: Pseudo dice [0.0003]
2024-05-30 22:06:08.018378: Epoch time: 184.26 s
2024-05-30 22:06:08.018471: Yayy! New best EMA pseudo Dice: 0.0003
2024-05-30 22:06:09.637856: 
2024-05-30 22:06:09.638061: Epoch 1
2024-05-30 22:06:09.638193: Current learning rate: 0.00491
2024-05-30 22:08:47.788573: train_loss -0.0874
2024-05-30 22:08:47.793007: val_loss -0.1275
2024-05-30 22:08:47.793127: Pseudo dice [0.0511]
2024-05-30 22:08:47.793260: Epoch time: 158.15 s
2024-05-30 22:08:47.793354: Yayy! New best EMA pseudo Dice: 0.0054
2024-05-30 22:08:49.578407: 
2024-05-30 22:08:49.578597: Epoch 2
2024-05-30 22:08:49.578726: Current learning rate: 0.00482
2024-05-30 22:11:25.322780: train_loss -0.2352
2024-05-30 22:11:25.324466: val_loss -0.2314
2024-05-30 22:11:25.324593: Pseudo dice [0.2717]
2024-05-30 22:11:25.324705: Epoch time: 155.75 s
2024-05-30 22:11:25.324793: Yayy! New best EMA pseudo Dice: 0.032
2024-05-30 22:11:27.149055: 
2024-05-30 22:11:27.149246: Epoch 3
2024-05-30 22:11:27.149368: Current learning rate: 0.00473
2024-05-30 22:14:05.177004: train_loss -0.2779
2024-05-30 22:14:05.179010: val_loss -0.2795
2024-05-30 22:14:05.179121: Pseudo dice [0.3227]
2024-05-30 22:14:05.179250: Epoch time: 158.03 s
2024-05-30 22:14:05.179340: Yayy! New best EMA pseudo Dice: 0.0611
2024-05-30 22:14:06.973588: 
2024-05-30 22:14:06.973817: Epoch 4
2024-05-30 22:14:06.973958: Current learning rate: 0.00464
2024-05-30 22:16:49.051538: train_loss -0.3017
2024-05-30 22:16:49.052758: val_loss -0.223
2024-05-30 22:16:49.052866: Pseudo dice [0.2648]
2024-05-30 22:16:49.052962: Epoch time: 162.08 s
2024-05-30 22:16:49.053105: Yayy! New best EMA pseudo Dice: 0.0815
2024-05-30 22:16:50.881155: 
2024-05-30 22:16:50.881355: Epoch 5
2024-05-30 22:16:50.881525: Current learning rate: 0.00455
2024-05-30 22:19:35.011866: train_loss -0.3332
2024-05-30 22:19:35.012810: val_loss -0.2673
2024-05-30 22:19:35.012923: Pseudo dice [0.3143]
2024-05-30 22:19:35.013030: Epoch time: 164.13 s
2024-05-30 22:19:35.013116: Yayy! New best EMA pseudo Dice: 0.1047
2024-05-30 22:19:36.756230: 
2024-05-30 22:19:36.756405: Epoch 6
2024-05-30 22:19:36.756526: Current learning rate: 0.00446
2024-05-30 22:22:18.657222: train_loss -0.3258
2024-05-30 22:22:18.657961: val_loss -0.3132
2024-05-30 22:22:18.658078: Pseudo dice [0.3629]
2024-05-30 22:22:18.658188: Epoch time: 161.9 s
2024-05-30 22:22:18.658277: Yayy! New best EMA pseudo Dice: 0.1306
2024-05-30 22:22:20.432531: 
2024-05-30 22:22:20.432718: Epoch 7
2024-05-30 22:22:20.432839: Current learning rate: 0.00437
2024-05-30 22:25:02.774236: train_loss -0.347
2024-05-30 22:25:02.776289: val_loss -0.2795
2024-05-30 22:25:02.776403: Pseudo dice [0.3194]
2024-05-30 22:25:02.776536: Epoch time: 162.34 s
2024-05-30 22:25:02.776634: Yayy! New best EMA pseudo Dice: 0.1494
2024-05-30 22:25:04.563678: 
2024-05-30 22:25:04.563857: Epoch 8
2024-05-30 22:25:04.563979: Current learning rate: 0.00427
2024-05-30 22:27:55.107857: train_loss -0.3448
2024-05-30 22:27:55.108565: val_loss -0.305
2024-05-30 22:27:55.108682: Pseudo dice [0.3424]
2024-05-30 22:27:55.108789: Epoch time: 170.55 s
2024-05-30 22:27:55.108874: Yayy! New best EMA pseudo Dice: 0.1687
2024-05-30 22:27:56.907226: 
2024-05-30 22:27:56.907419: Epoch 9
2024-05-30 22:27:56.907542: Current learning rate: 0.00418
2024-05-30 22:30:55.399616: train_loss -0.3718
2024-05-30 22:30:55.401866: val_loss -0.3026
2024-05-30 22:30:55.401979: Pseudo dice [0.347]
2024-05-30 22:30:55.402112: Epoch time: 178.49 s
2024-05-30 22:30:55.402202: Yayy! New best EMA pseudo Dice: 0.1866
2024-05-30 22:30:57.136879: 
2024-05-30 22:30:57.137065: Epoch 10
2024-05-30 22:30:57.137187: Current learning rate: 0.00409
2024-05-30 22:33:46.871842: train_loss -0.3641
2024-05-30 22:33:46.872500: val_loss -0.347
2024-05-30 22:33:46.872619: Pseudo dice [0.3874]
2024-05-30 22:33:46.872722: Epoch time: 169.74 s
2024-05-30 22:33:46.872808: Yayy! New best EMA pseudo Dice: 0.2067
2024-05-30 22:33:48.609727: 
2024-05-30 22:33:48.609947: Epoch 11
2024-05-30 22:33:48.610074: Current learning rate: 0.004
2024-05-30 22:36:34.351686: train_loss -0.3853
2024-05-30 22:36:34.363714: val_loss -0.3372
2024-05-30 22:36:34.363858: Pseudo dice [0.3822]
2024-05-30 22:36:34.364005: Epoch time: 165.74 s
2024-05-30 22:36:34.364100: Yayy! New best EMA pseudo Dice: 0.2242
2024-05-30 22:36:36.173563: 
2024-05-30 22:36:36.173761: Epoch 12
2024-05-30 22:36:36.173889: Current learning rate: 0.00391
2024-05-30 22:39:20.763976: train_loss -0.3757
2024-05-30 22:39:20.764645: val_loss -0.3164
2024-05-30 22:39:20.764764: Pseudo dice [0.3499]
2024-05-30 22:39:20.764881: Epoch time: 164.59 s
2024-05-30 22:39:20.764974: Yayy! New best EMA pseudo Dice: 0.2368
2024-05-30 22:39:22.533537: 
2024-05-30 22:39:22.533728: Epoch 13
2024-05-30 22:39:22.533853: Current learning rate: 0.00381
2024-05-30 22:42:06.418738: train_loss -0.3832
2024-05-30 22:42:06.420904: val_loss -0.3224
2024-05-30 22:42:06.422123: Pseudo dice [0.3683]
2024-05-30 22:42:06.422258: Epoch time: 163.89 s
2024-05-30 22:42:06.422353: Yayy! New best EMA pseudo Dice: 0.2499
2024-05-30 22:42:08.212767: 
2024-05-30 22:42:08.212944: Epoch 14
2024-05-30 22:42:08.213068: Current learning rate: 0.00372
2024-05-30 22:44:47.642658: train_loss -0.4038
2024-05-30 22:44:47.643350: val_loss -0.315
2024-05-30 22:44:47.643460: Pseudo dice [0.3701]
2024-05-30 22:44:47.643560: Epoch time: 159.43 s
2024-05-30 22:44:47.643654: Yayy! New best EMA pseudo Dice: 0.2619
2024-05-30 22:44:49.610583: 
2024-05-30 22:44:49.610932: Epoch 15
2024-05-30 22:44:49.611247: Current learning rate: 0.00363
2024-05-30 22:47:35.549429: train_loss -0.4069
2024-05-30 22:47:35.551561: val_loss -0.3429
2024-05-30 22:47:35.551682: Pseudo dice [0.3814]
2024-05-30 22:47:35.551813: Epoch time: 165.94 s
2024-05-30 22:47:35.551906: Yayy! New best EMA pseudo Dice: 0.2739
2024-05-30 22:47:37.347035: 
2024-05-30 22:47:37.347210: Epoch 16
2024-05-30 22:47:37.347333: Current learning rate: 0.00353
2024-05-30 22:50:20.315867: train_loss -0.4127
2024-05-30 22:50:20.318035: val_loss -0.333
2024-05-30 22:50:20.318149: Pseudo dice [0.3819]
2024-05-30 22:50:20.318248: Epoch time: 162.97 s
2024-05-30 22:50:20.318331: Yayy! New best EMA pseudo Dice: 0.2847
2024-05-30 22:50:22.144398: 
2024-05-30 22:50:22.144582: Epoch 17
2024-05-30 22:50:22.144706: Current learning rate: 0.00344
2024-05-30 22:53:08.172899: train_loss -0.4216
2024-05-30 22:53:08.177587: val_loss -0.3595
2024-05-30 22:53:08.177692: Pseudo dice [0.4026]
2024-05-30 22:53:08.177811: Epoch time: 166.03 s
2024-05-30 22:53:08.177898: Yayy! New best EMA pseudo Dice: 0.2965
2024-05-30 22:53:10.012200: 
2024-05-30 22:53:10.012453: Epoch 18
2024-05-30 22:53:10.012624: Current learning rate: 0.00335
2024-05-30 22:55:55.861586: train_loss -0.416
2024-05-30 22:55:55.862606: val_loss -0.3243
2024-05-30 22:55:55.862719: Pseudo dice [0.3642]
2024-05-30 22:55:55.862823: Epoch time: 165.85 s
2024-05-30 22:55:55.862909: Yayy! New best EMA pseudo Dice: 0.3033
2024-05-30 22:55:57.642351: 
2024-05-30 22:55:57.642526: Epoch 19
2024-05-30 22:55:57.642653: Current learning rate: 0.00325
2024-05-30 22:58:38.816438: train_loss -0.4174
2024-05-30 22:58:38.818459: val_loss -0.347
2024-05-30 22:58:38.818571: Pseudo dice [0.3928]
2024-05-30 22:58:38.818707: Epoch time: 161.18 s
2024-05-30 22:58:38.818800: Yayy! New best EMA pseudo Dice: 0.3122
2024-05-30 22:58:40.626852: 
2024-05-30 22:58:40.627036: Epoch 20
2024-05-30 22:58:40.627160: Current learning rate: 0.00316
2024-05-30 23:01:20.813129: train_loss -0.4398
2024-05-30 23:01:20.813784: val_loss -0.3824
2024-05-30 23:01:20.813899: Pseudo dice [0.4372]
2024-05-30 23:01:20.814003: Epoch time: 160.19 s
2024-05-30 23:01:20.814088: Yayy! New best EMA pseudo Dice: 0.3247
2024-05-30 23:01:22.603930: 
2024-05-30 23:01:22.604234: Epoch 21
2024-05-30 23:01:22.604360: Current learning rate: 0.00306
2024-05-30 23:06:08.231145: train_loss -0.427
2024-05-30 23:06:08.233355: val_loss -0.3435
2024-05-30 23:06:08.233464: Pseudo dice [0.3807]
2024-05-30 23:06:08.233603: Epoch time: 285.63 s
2024-05-30 23:06:08.233695: Yayy! New best EMA pseudo Dice: 0.3303
2024-05-30 23:06:09.961386: 
2024-05-30 23:06:09.961568: Epoch 22
2024-05-30 23:06:09.961695: Current learning rate: 0.00297
2024-05-30 23:08:55.216887: train_loss -0.4423
2024-05-30 23:08:55.217725: val_loss -0.4089
2024-05-30 23:08:55.217840: Pseudo dice [0.4585]
2024-05-30 23:08:55.217950: Epoch time: 165.26 s
2024-05-30 23:08:55.218034: Yayy! New best EMA pseudo Dice: 0.3431
2024-05-30 23:08:56.956867: 
2024-05-30 23:08:56.957047: Epoch 23
2024-05-30 23:08:56.957169: Current learning rate: 0.00287
2024-05-30 23:11:37.441763: train_loss -0.4364
2024-05-30 23:11:37.443816: val_loss -0.3834
2024-05-30 23:11:37.443928: Pseudo dice [0.4343]
2024-05-30 23:11:37.444052: Epoch time: 160.49 s
2024-05-30 23:11:37.444140: Yayy! New best EMA pseudo Dice: 0.3522
2024-05-30 23:11:39.155666: 
2024-05-30 23:11:39.155849: Epoch 24
2024-05-30 23:11:39.155970: Current learning rate: 0.00278
2024-05-30 23:14:18.539918: train_loss -0.436
2024-05-30 23:14:18.541044: val_loss -0.3636
2024-05-30 23:14:18.541156: Pseudo dice [0.4113]
2024-05-30 23:14:18.541264: Epoch time: 159.39 s
2024-05-30 23:14:18.541346: Yayy! New best EMA pseudo Dice: 0.3581
2024-05-30 23:14:20.262302: 
2024-05-30 23:14:20.262496: Epoch 25
2024-05-30 23:14:20.262623: Current learning rate: 0.00268
2024-05-30 23:16:59.331950: train_loss -0.4557
2024-05-30 23:16:59.336246: val_loss -0.3486
2024-05-30 23:16:59.336357: Pseudo dice [0.396]
2024-05-30 23:16:59.336479: Epoch time: 159.07 s
2024-05-30 23:16:59.336566: Yayy! New best EMA pseudo Dice: 0.3619
2024-05-30 23:17:01.094496: 
2024-05-30 23:17:01.094686: Epoch 26
2024-05-30 23:17:01.094809: Current learning rate: 0.00258
2024-05-30 23:19:47.379501: train_loss -0.4419
2024-05-30 23:19:47.380224: val_loss -0.3042
2024-05-30 23:19:47.380337: Pseudo dice [0.3454]
2024-05-30 23:19:47.380448: Epoch time: 166.29 s
2024-05-30 23:19:48.863091: 
2024-05-30 23:19:48.863267: Epoch 27
2024-05-30 23:19:48.863390: Current learning rate: 0.00249
2024-05-30 23:22:33.242277: train_loss -0.4608
2024-05-30 23:22:33.246714: val_loss -0.3687
2024-05-30 23:22:33.246824: Pseudo dice [0.4179]
2024-05-30 23:22:33.246922: Epoch time: 164.38 s
2024-05-30 23:22:33.247004: Yayy! New best EMA pseudo Dice: 0.366
2024-05-30 23:22:35.040168: 
2024-05-30 23:22:35.040357: Epoch 28
2024-05-30 23:22:35.040478: Current learning rate: 0.00239
2024-05-30 23:25:14.749531: train_loss -0.4552
2024-05-30 23:25:14.753907: val_loss -0.3619
2024-05-30 23:25:14.754017: Pseudo dice [0.397]
2024-05-30 23:25:14.754123: Epoch time: 159.71 s
2024-05-30 23:25:14.754204: Yayy! New best EMA pseudo Dice: 0.3691
2024-05-30 23:25:16.479051: 
2024-05-30 23:25:16.479230: Epoch 29
2024-05-30 23:25:16.479354: Current learning rate: 0.00229
2024-05-30 23:28:00.618155: train_loss -0.4585
2024-05-30 23:28:00.622718: val_loss -0.3475
2024-05-30 23:28:00.622835: Pseudo dice [0.4027]
2024-05-30 23:28:00.622948: Epoch time: 164.14 s
2024-05-30 23:28:00.623033: Yayy! New best EMA pseudo Dice: 0.3725
2024-05-30 23:28:02.816367: 
2024-05-30 23:28:02.816571: Epoch 30
2024-05-30 23:28:02.816702: Current learning rate: 0.00219
2024-05-30 23:30:46.727285: train_loss -0.4894
2024-05-30 23:30:46.732393: val_loss -0.3637
2024-05-30 23:30:46.732497: Pseudo dice [0.4056]
2024-05-30 23:30:46.732597: Epoch time: 163.91 s
2024-05-30 23:30:46.732681: Yayy! New best EMA pseudo Dice: 0.3758
2024-05-30 23:30:48.510254: 
2024-05-30 23:30:48.510436: Epoch 31
2024-05-30 23:30:48.510561: Current learning rate: 0.00209
2024-05-30 23:33:30.719969: train_loss -0.4648
2024-05-30 23:33:30.732019: val_loss -0.3715
2024-05-30 23:33:30.732140: Pseudo dice [0.4135]
2024-05-30 23:33:30.732293: Epoch time: 162.21 s
2024-05-30 23:33:30.732381: Yayy! New best EMA pseudo Dice: 0.3796
2024-05-30 23:33:32.586647: 
2024-05-30 23:33:32.586825: Epoch 32
2024-05-30 23:33:32.586947: Current learning rate: 0.00199
2024-05-30 23:36:19.092330: train_loss -0.4708
2024-05-30 23:36:19.096908: val_loss -0.3886
2024-05-30 23:36:19.097062: Pseudo dice [0.4315]
2024-05-30 23:36:19.097219: Epoch time: 166.51 s
2024-05-30 23:36:19.097322: Yayy! New best EMA pseudo Dice: 0.3848
2024-05-30 23:36:20.945294: 
2024-05-30 23:36:20.945477: Epoch 33
2024-05-30 23:36:20.945604: Current learning rate: 0.00189
2024-05-30 23:39:10.709307: train_loss -0.4772
2024-05-30 23:39:10.711137: val_loss -0.3735
2024-05-30 23:39:10.711246: Pseudo dice [0.4208]
2024-05-30 23:39:10.711377: Epoch time: 169.77 s
2024-05-30 23:39:10.711465: Yayy! New best EMA pseudo Dice: 0.3884
2024-05-30 23:39:12.594381: 
2024-05-30 23:39:12.594551: Epoch 34
2024-05-30 23:39:12.594676: Current learning rate: 0.00179
2024-05-30 23:42:05.243701: train_loss -0.4704
2024-05-30 23:42:05.248535: val_loss -0.3729
2024-05-30 23:42:05.248655: Pseudo dice [0.419]
2024-05-30 23:42:05.249676: Epoch time: 172.65 s
2024-05-30 23:42:05.249769: Yayy! New best EMA pseudo Dice: 0.3914
2024-05-30 23:42:07.095742: 
2024-05-30 23:42:07.095920: Epoch 35
2024-05-30 23:42:07.096043: Current learning rate: 0.00169
2024-05-30 23:44:51.621345: train_loss -0.4837
2024-05-30 23:44:51.625713: val_loss -0.3336
2024-05-30 23:44:51.625827: Pseudo dice [0.3906]
2024-05-30 23:44:51.625956: Epoch time: 164.53 s
2024-05-30 23:44:53.174601: 
2024-05-30 23:44:53.174774: Epoch 36
2024-05-30 23:44:53.174897: Current learning rate: 0.00159
2024-05-30 23:47:38.595278: train_loss -0.4812
2024-05-30 23:47:38.599550: val_loss -0.3378
2024-05-30 23:47:38.599669: Pseudo dice [0.3784]
2024-05-30 23:47:38.599801: Epoch time: 165.42 s
2024-05-30 23:47:40.137239: 
2024-05-30 23:47:40.137410: Epoch 37
2024-05-30 23:47:40.137760: Current learning rate: 0.00149
2024-05-30 23:50:22.832752: train_loss -0.4818
2024-05-30 23:50:22.837601: val_loss -0.3612
2024-05-30 23:50:22.837760: Pseudo dice [0.4078]
2024-05-30 23:50:22.837947: Epoch time: 162.7 s
2024-05-30 23:50:22.838042: Yayy! New best EMA pseudo Dice: 0.3918
2024-05-30 23:50:24.991465: 
2024-05-30 23:50:24.991651: Epoch 38
2024-05-30 23:50:24.991773: Current learning rate: 0.00138
2024-05-30 23:53:35.399046: train_loss -0.495
2024-05-30 23:53:35.403360: val_loss -0.3621
2024-05-30 23:53:35.403474: Pseudo dice [0.4002]
2024-05-30 23:53:35.403630: Epoch time: 190.41 s
2024-05-30 23:53:35.403747: Yayy! New best EMA pseudo Dice: 0.3927
2024-05-30 23:53:37.267329: 
2024-05-30 23:53:37.267498: Epoch 39
2024-05-30 23:53:37.267624: Current learning rate: 0.00128
2024-05-30 23:56:39.408826: train_loss -0.502
2024-05-30 23:56:39.409732: val_loss -0.3726
2024-05-30 23:56:39.409877: Pseudo dice [0.4163]
2024-05-30 23:56:39.410038: Epoch time: 182.14 s
2024-05-30 23:56:39.410128: Yayy! New best EMA pseudo Dice: 0.395
2024-05-30 23:56:41.327343: 
2024-05-30 23:56:41.327539: Epoch 40
2024-05-30 23:56:41.327671: Current learning rate: 0.00117
2024-05-30 23:59:22.465738: train_loss -0.5001
2024-05-30 23:59:22.470352: val_loss -0.4194
2024-05-30 23:59:22.470467: Pseudo dice [0.4591]
2024-05-30 23:59:22.470607: Epoch time: 161.14 s
2024-05-30 23:59:22.470696: Yayy! New best EMA pseudo Dice: 0.4014
2024-05-30 23:59:24.290908: 
2024-05-30 23:59:24.291081: Epoch 41
2024-05-30 23:59:24.291204: Current learning rate: 0.00107
slurmstepd: error: *** JOB 6443845 ON gcn64 CANCELLED AT 2024-05-31T00:00:57 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: open failed for /slurm/6443845/.ns: No such file or directory
slurmstepd: error: container_g_join(6443845): No such file or directory

JOB STATISTICS
==============
Job ID: 6443845
Cluster: snellius
User/Group: ljulius/ljulius
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:24
CPU Efficiency: 0.02% of 1-12:07:12 core-walltime
Job Wall-clock time: 02:00:24
Memory Utilized: 7.24 GB
Memory Efficiency: 72.42% of 10.00 GB
