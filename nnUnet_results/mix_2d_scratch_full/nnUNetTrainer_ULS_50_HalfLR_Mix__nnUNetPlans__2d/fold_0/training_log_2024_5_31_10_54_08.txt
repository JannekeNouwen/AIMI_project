
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': [256, 256], 'median_image_size_in_voxels': [255.0, 255.0], 'spacing': [0.7578125, 0.7578125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [6, 6], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_seg': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'no_resampling_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset501_RadboudumcBone', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.800000011920929, 0.7578125, 0.7578125], 'original_median_shape_after_transp': [128, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlannerNoResampling', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2782.0, 'mean': 1175.083251953125, 'median': 1230.0, 'min': -935.0, 'percentile_00_5': 33.0, 'percentile_99_5': 2269.0, 'std': 513.393310546875}}} 
 
2024-05-31 10:54:09.928956: unpacking dataset... 
2024-05-31 10:54:39.420504: unpacking done... 
2024-05-31 10:54:39.429019: do_dummy_2d_data_aug: False 
2024-05-31 10:54:39.459091: Using splits from existing split file: /scratch-local/ljulius.6454231/nnUNet_preprocessed/Dataset501_RadboudumcBone/splits_final.json 
2024-05-31 10:54:39.460007: The split file contains 5 splits. 
2024-05-31 10:54:39.460109: Desired fold for training: 0 
2024-05-31 10:54:39.460175: This split has 557 training and 140 validation cases. 
2024-05-31 10:54:40.182637: Unable to plot network architecture: 
2024-05-31 10:54:40.182794: No module named 'IPython' 
2024-05-31 10:54:40.233262:  
2024-05-31 10:54:40.233424: Epoch 0 
2024-05-31 10:54:40.233633: Current learning rate: 0.005 
2024-05-31 11:01:19.000572: train_loss -0.2169 
2024-05-31 11:01:19.001179: val_loss -0.306 
2024-05-31 11:01:19.001292: Pseudo dice [0.3694] 
2024-05-31 11:01:19.001399: Epoch time: 398.77 s 
2024-05-31 11:01:19.001496: Yayy! New best EMA pseudo Dice: 0.3694 
2024-05-31 11:01:20.676372:  
2024-05-31 11:01:20.676571: Epoch 1 
2024-05-31 11:01:20.676701: Current learning rate: 0.00491 
2024-05-31 11:09:24.734117: train_loss -0.4299 
2024-05-31 11:09:24.750422: val_loss -0.3195 
2024-05-31 11:09:24.750561: Pseudo dice [0.3655] 
2024-05-31 11:09:24.750699: Epoch time: 484.06 s 
2024-05-31 11:09:26.222482:  
2024-05-31 11:09:26.222679: Epoch 2 
2024-05-31 11:09:26.222803: Current learning rate: 0.00482 
2024-05-31 11:14:33.061636: train_loss -0.4503 
2024-05-31 11:14:33.062150: val_loss -0.3753 
2024-05-31 11:14:33.063762: Pseudo dice [0.4216] 
2024-05-31 11:14:33.063870: Epoch time: 306.84 s 
2024-05-31 11:14:33.063954: Yayy! New best EMA pseudo Dice: 0.3743 
2024-05-31 11:14:34.894210:  
2024-05-31 11:14:34.894401: Epoch 3 
2024-05-31 11:14:34.894529: Current learning rate: 0.00473 
2024-05-31 11:19:01.156731: train_loss -0.4735 
2024-05-31 11:19:01.166767: val_loss -0.3412 
2024-05-31 11:19:01.166884: Pseudo dice [0.3747] 
2024-05-31 11:19:01.167013: Epoch time: 266.26 s 
2024-05-31 11:19:01.167101: Yayy! New best EMA pseudo Dice: 0.3743 
2024-05-31 11:19:02.944977:  
2024-05-31 11:19:02.945482: Epoch 4 
2024-05-31 11:19:02.945605: Current learning rate: 0.00464 
2024-05-31 11:25:15.517594: train_loss -0.4533 
2024-05-31 11:25:15.518260: val_loss -0.3802 
2024-05-31 11:25:15.518371: Pseudo dice [0.4234] 
2024-05-31 11:25:15.518486: Epoch time: 372.57 s 
2024-05-31 11:25:15.518572: Yayy! New best EMA pseudo Dice: 0.3793 
2024-05-31 11:25:17.492165:  
2024-05-31 11:25:17.492342: Epoch 5 
2024-05-31 11:25:17.492470: Current learning rate: 0.00455 
2024-05-31 11:29:26.489015: train_loss -0.4582 
2024-05-31 11:29:26.498108: val_loss -0.3863 
2024-05-31 11:29:26.498222: Pseudo dice [0.423] 
2024-05-31 11:29:26.498360: Epoch time: 249.0 s 
2024-05-31 11:29:26.498448: Yayy! New best EMA pseudo Dice: 0.3836 
2024-05-31 11:29:28.616306:  
2024-05-31 11:29:28.616493: Epoch 6 
2024-05-31 11:29:28.616614: Current learning rate: 0.00446 
2024-05-31 11:36:48.389175: train_loss -0.4581 
2024-05-31 11:36:48.390145: val_loss -0.3377 
2024-05-31 11:36:48.390255: Pseudo dice [0.3772] 
2024-05-31 11:36:48.390359: Epoch time: 439.77 s 
2024-05-31 11:36:49.915301:  
2024-05-31 11:36:49.915509: Epoch 7 
2024-05-31 11:36:49.915636: Current learning rate: 0.00437 
2024-05-31 11:40:31.839549: train_loss -0.4754 
2024-05-31 11:40:31.849864: val_loss -0.3413 
2024-05-31 11:40:31.849976: Pseudo dice [0.3831] 
2024-05-31 11:40:31.850104: Epoch time: 221.93 s 
2024-05-31 11:40:33.394280:  
2024-05-31 11:40:33.394483: Epoch 8 
2024-05-31 11:40:33.394608: Current learning rate: 0.00427 
2024-05-31 11:44:24.997566: train_loss -0.4964 
2024-05-31 11:44:24.998386: val_loss -0.3146 
2024-05-31 11:44:24.998501: Pseudo dice [0.3488] 
2024-05-31 11:44:24.998600: Epoch time: 231.6 s 
2024-05-31 11:44:26.592716:  
2024-05-31 11:44:26.593159: Epoch 9 
2024-05-31 11:44:26.593301: Current learning rate: 0.00418 
2024-05-31 11:48:10.986462: train_loss -0.4933 
2024-05-31 11:48:10.995607: val_loss -0.3514 
2024-05-31 11:48:10.995716: Pseudo dice [0.3905] 
2024-05-31 11:48:10.995845: Epoch time: 224.4 s 
2024-05-31 11:48:12.494922:  
2024-05-31 11:48:12.495099: Epoch 10 
2024-05-31 11:48:12.495219: Current learning rate: 0.00409 
2024-05-31 11:52:54.649549: train_loss -0.4903 
2024-05-31 11:52:54.650306: val_loss -0.3828 
2024-05-31 11:52:54.650417: Pseudo dice [0.4271] 
2024-05-31 11:52:54.650533: Epoch time: 282.16 s 
2024-05-31 11:52:54.650619: Yayy! New best EMA pseudo Dice: 0.3853 
2024-05-31 11:52:56.436065:  
2024-05-31 11:52:56.436260: Epoch 11 
2024-05-31 11:52:56.436385: Current learning rate: 0.004 
2024-05-31 11:56:52.032223: train_loss -0.4951 
2024-05-31 11:56:52.038435: val_loss -0.3901 
2024-05-31 11:56:52.038548: Pseudo dice [0.4313] 
2024-05-31 11:56:52.038675: Epoch time: 235.6 s 
2024-05-31 11:56:52.038764: Yayy! New best EMA pseudo Dice: 0.3899 
2024-05-31 11:56:53.830627:  
2024-05-31 11:56:53.830859: Epoch 12 
2024-05-31 11:56:53.830990: Current learning rate: 0.00391 
2024-05-31 12:00:12.361323: train_loss -0.5087 
2024-05-31 12:00:12.362099: val_loss -0.3311 
2024-05-31 12:00:12.362212: Pseudo dice [0.3661] 
2024-05-31 12:00:12.362311: Epoch time: 198.53 s 
2024-05-31 12:00:13.906250:  
2024-05-31 12:00:13.906893: Epoch 13 
2024-05-31 12:00:13.907022: Current learning rate: 0.00381 
2024-05-31 12:05:46.528391: train_loss -0.5061 
2024-05-31 12:05:46.538007: val_loss -0.407 
2024-05-31 12:05:46.538128: Pseudo dice [0.4542] 
2024-05-31 12:05:46.538261: Epoch time: 332.62 s 
2024-05-31 12:05:46.538351: Yayy! New best EMA pseudo Dice: 0.3942 
2024-05-31 12:05:48.363498:  
2024-05-31 12:05:48.364023: Epoch 14 
2024-05-31 12:05:48.364154: Current learning rate: 0.00372 
2024-05-31 12:09:21.950977: train_loss -0.5115 
2024-05-31 12:09:21.951694: val_loss -0.3849 
2024-05-31 12:09:21.954059: Pseudo dice [0.4448] 
2024-05-31 12:09:21.954161: Epoch time: 213.59 s 
2024-05-31 12:09:21.954245: Yayy! New best EMA pseudo Dice: 0.3993 
2024-05-31 12:09:23.766042:  
2024-05-31 12:09:23.766237: Epoch 15 
2024-05-31 12:09:23.766360: Current learning rate: 0.00363 
2024-05-31 12:12:53.056632: train_loss -0.5174 
2024-05-31 12:12:53.061885: val_loss -0.3308 
2024-05-31 12:12:53.061996: Pseudo dice [0.3744] 
2024-05-31 12:12:53.062121: Epoch time: 209.29 s 
2024-05-31 12:12:54.629726:  
2024-05-31 12:12:54.630144: Epoch 16 
2024-05-31 12:12:54.630269: Current learning rate: 0.00353 
2024-05-31 12:17:11.570653: train_loss -0.5262 
2024-05-31 12:17:11.571427: val_loss -0.349 
2024-05-31 12:17:11.571545: Pseudo dice [0.3958] 
2024-05-31 12:17:11.571646: Epoch time: 256.94 s 
2024-05-31 12:17:13.158087:  
2024-05-31 12:17:13.158290: Epoch 17 
2024-05-31 12:17:13.158413: Current learning rate: 0.00344 
2024-05-31 12:21:46.856027: train_loss -0.5291 
2024-05-31 12:21:46.863691: val_loss -0.3341 
2024-05-31 12:21:46.863805: Pseudo dice [0.3729] 
2024-05-31 12:21:46.863926: Epoch time: 273.7 s 
2024-05-31 12:21:48.455221:  
2024-05-31 12:21:48.455782: Epoch 18 
2024-05-31 12:21:48.455912: Current learning rate: 0.00335 
2024-05-31 12:25:29.874501: train_loss -0.5227 
2024-05-31 12:25:29.875407: val_loss -0.3518 
2024-05-31 12:25:29.875523: Pseudo dice [0.4045] 
2024-05-31 12:25:29.876020: Epoch time: 221.42 s 
2024-05-31 12:25:31.563510:  
2024-05-31 12:25:31.563879: Epoch 19 
2024-05-31 12:25:31.564009: Current learning rate: 0.00325 
2024-05-31 12:29:06.325674: train_loss -0.5226 
2024-05-31 12:29:06.335587: val_loss -0.3541 
2024-05-31 12:29:06.335704: Pseudo dice [0.3972] 
2024-05-31 12:29:06.335833: Epoch time: 214.76 s 
2024-05-31 12:29:07.896265:  
2024-05-31 12:29:07.896663: Epoch 20 
2024-05-31 12:29:07.896798: Current learning rate: 0.00316 
2024-05-31 12:35:17.174572: train_loss -0.5411 
2024-05-31 12:35:17.184619: val_loss -0.436 
2024-05-31 12:35:17.184734: Pseudo dice [0.4782] 
2024-05-31 12:35:17.184837: Epoch time: 369.28 s 
2024-05-31 12:35:17.184920: Yayy! New best EMA pseudo Dice: 0.4038 
2024-05-31 12:35:19.069521:  
2024-05-31 12:35:19.070249: Epoch 21 
2024-05-31 12:35:19.070375: Current learning rate: 0.00306 
2024-05-31 12:39:08.829249: train_loss -0.5374 
2024-05-31 12:39:08.838509: val_loss -0.363 
2024-05-31 12:39:08.838624: Pseudo dice [0.4025] 
2024-05-31 12:39:08.838747: Epoch time: 229.76 s 
2024-05-31 12:39:10.370463:  
2024-05-31 12:39:10.370649: Epoch 22 
2024-05-31 12:39:10.370770: Current learning rate: 0.00297 
2024-05-31 12:42:27.430328: train_loss -0.5513 
2024-05-31 12:42:27.431136: val_loss -0.3596 
2024-05-31 12:42:27.431254: Pseudo dice [0.4125] 
2024-05-31 12:42:27.431359: Epoch time: 197.06 s 
2024-05-31 12:42:27.431443: Yayy! New best EMA pseudo Dice: 0.4045 
2024-05-31 12:42:29.340117:  
2024-05-31 12:42:29.340309: Epoch 23 
2024-05-31 12:42:29.340436: Current learning rate: 0.00287 
2024-05-31 12:49:17.993986: train_loss -0.5473 
2024-05-31 12:49:18.000263: val_loss -0.3698 
2024-05-31 12:49:18.000382: Pseudo dice [0.4071] 
2024-05-31 12:49:18.000515: Epoch time: 408.66 s 
2024-05-31 12:49:18.000602: Yayy! New best EMA pseudo Dice: 0.4048 
2024-05-31 12:49:19.978594:  
2024-05-31 12:49:19.978776: Epoch 24 
2024-05-31 12:49:19.978896: Current learning rate: 0.00278 
2024-05-31 12:53:37.310995: train_loss -0.5594 
2024-05-31 12:53:37.311656: val_loss -0.3748 
2024-05-31 12:53:37.311769: Pseudo dice [0.4125] 
2024-05-31 12:53:37.311872: Epoch time: 257.33 s 
2024-05-31 12:53:37.311955: Yayy! New best EMA pseudo Dice: 0.4056 
2024-05-31 12:53:39.102666:  
2024-05-31 12:53:39.102857: Epoch 25 
2024-05-31 12:53:39.102978: Current learning rate: 0.00268 
2024-05-31 12:57:00.198964: train_loss -0.5503 
2024-05-31 12:57:00.199780: val_loss -0.4143 
2024-05-31 12:57:00.199895: Pseudo dice [0.4583] 
2024-05-31 12:57:00.200001: Epoch time: 201.1 s 
2024-05-31 12:57:00.200084: Yayy! New best EMA pseudo Dice: 0.4108 
2024-05-31 12:57:02.063556:  
2024-05-31 12:57:02.064411: Epoch 26 
2024-05-31 12:57:02.064548: Current learning rate: 0.00258 
2024-05-31 13:02:14.882672: train_loss -0.5572 
2024-05-31 13:02:14.884016: val_loss -0.4246 
2024-05-31 13:02:14.884129: Pseudo dice [0.4714] 
2024-05-31 13:02:14.884237: Epoch time: 312.82 s 
2024-05-31 13:02:14.884321: Yayy! New best EMA pseudo Dice: 0.4169 
2024-05-31 13:02:16.757940:  
2024-05-31 13:02:16.758123: Epoch 27 
2024-05-31 13:02:16.758245: Current learning rate: 0.00249 
2024-05-31 13:06:04.167730: train_loss -0.5689 
2024-05-31 13:06:04.168545: val_loss -0.3459 
2024-05-31 13:06:04.168659: Pseudo dice [0.3857] 
2024-05-31 13:06:04.168761: Epoch time: 227.41 s 
2024-05-31 13:06:05.816537:  
2024-05-31 13:06:05.816923: Epoch 28 
2024-05-31 13:06:05.817044: Current learning rate: 0.00239 
2024-05-31 13:09:31.909729: train_loss -0.5747 
2024-05-31 13:09:31.910449: val_loss -0.4116 
2024-05-31 13:09:31.910562: Pseudo dice [0.4522] 
2024-05-31 13:09:31.910660: Epoch time: 206.09 s 
2024-05-31 13:09:31.910743: Yayy! New best EMA pseudo Dice: 0.4176 
2024-05-31 13:09:33.922856:  
2024-05-31 13:09:33.923055: Epoch 29 
2024-05-31 13:09:33.923175: Current learning rate: 0.00229 
2024-05-31 13:14:28.562844: train_loss -0.5724 
2024-05-31 13:14:28.571717: val_loss -0.3599 
2024-05-31 13:14:28.571835: Pseudo dice [0.3984] 
2024-05-31 13:14:28.571963: Epoch time: 294.64 s 
2024-05-31 13:14:30.146041:  
2024-05-31 13:14:30.146243: Epoch 30 
2024-05-31 13:14:30.146368: Current learning rate: 0.00219 
2024-05-31 13:17:59.413990: train_loss -0.5667 
2024-05-31 13:17:59.425303: val_loss -0.3793 
2024-05-31 13:17:59.425426: Pseudo dice [0.4263] 
2024-05-31 13:17:59.425605: Epoch time: 209.27 s 
2024-05-31 13:18:00.982207:  
2024-05-31 13:18:00.982400: Epoch 31 
2024-05-31 13:18:00.982533: Current learning rate: 0.00209 
2024-05-31 13:21:18.220526: train_loss -0.5674 
2024-05-31 13:21:18.230632: val_loss -0.3185 
2024-05-31 13:21:18.230742: Pseudo dice [0.354] 
2024-05-31 13:21:18.230861: Epoch time: 197.24 s 
2024-05-31 13:21:19.905004:  
2024-05-31 13:21:19.905217: Epoch 32 
2024-05-31 13:21:19.905344: Current learning rate: 0.00199 
2024-05-31 13:25:33.919755: train_loss -0.5854 
2024-05-31 13:25:33.929159: val_loss -0.3622 
2024-05-31 13:25:33.929286: Pseudo dice [0.4006] 
2024-05-31 13:25:33.929427: Epoch time: 254.02 s 
2024-05-31 13:25:35.517137:  
2024-05-31 13:25:35.517330: Epoch 33 
2024-05-31 13:25:35.517453: Current learning rate: 0.00189 
2024-05-31 13:29:03.803191: train_loss -0.5847 
2024-05-31 13:29:03.813551: val_loss -0.3536 
2024-05-31 13:29:03.813676: Pseudo dice [0.3935] 
2024-05-31 13:29:03.813816: Epoch time: 208.29 s 
2024-05-31 13:29:05.412384:  
2024-05-31 13:29:05.412803: Epoch 34 
2024-05-31 13:29:05.412935: Current learning rate: 0.00179 
2024-05-31 13:32:16.857924: train_loss -0.5926 
2024-05-31 13:32:16.868649: val_loss -0.3899 
2024-05-31 13:32:16.868869: Pseudo dice [0.4348] 
2024-05-31 13:32:16.869123: Epoch time: 191.45 s 
2024-05-31 13:32:18.483287:  
2024-05-31 13:32:18.483526: Epoch 35 
2024-05-31 13:32:18.483703: Current learning rate: 0.00169 
2024-05-31 13:35:33.440379: train_loss -0.5876 
2024-05-31 13:35:33.451798: val_loss -0.3146 
2024-05-31 13:35:33.451940: Pseudo dice [0.3562] 
2024-05-31 13:35:33.452085: Epoch time: 194.96 s 
2024-05-31 13:35:35.041288:  
2024-05-31 13:35:35.041467: Epoch 36 
2024-05-31 13:35:35.041587: Current learning rate: 0.00159 
2024-05-31 13:39:05.971032: train_loss -0.5934 
2024-05-31 13:39:05.984205: val_loss -0.3611 
2024-05-31 13:39:05.984330: Pseudo dice [0.4111] 
2024-05-31 13:39:05.984478: Epoch time: 210.93 s 
2024-05-31 13:39:07.607942:  
2024-05-31 13:39:07.608126: Epoch 37 
2024-05-31 13:39:07.608248: Current learning rate: 0.00149 
2024-05-31 13:42:32.080504: train_loss -0.5966 
2024-05-31 13:42:32.092963: val_loss -0.4292 
2024-05-31 13:42:32.093082: Pseudo dice [0.4837] 
2024-05-31 13:42:32.093213: Epoch time: 204.47 s 
2024-05-31 13:42:33.679561:  
2024-05-31 13:42:33.679756: Epoch 38 
2024-05-31 13:42:33.679876: Current learning rate: 0.00138 
2024-05-31 13:45:52.778155: train_loss -0.6095 
2024-05-31 13:45:52.789170: val_loss -0.3892 
2024-05-31 13:45:52.789289: Pseudo dice [0.4385] 
2024-05-31 13:45:52.789411: Epoch time: 199.1 s 
2024-05-31 13:45:54.417789:  
2024-05-31 13:45:54.418561: Epoch 39 
2024-05-31 13:45:54.418761: Current learning rate: 0.00128 
2024-05-31 13:49:08.135946: train_loss -0.6098 
2024-05-31 13:49:08.144597: val_loss -0.4259 
2024-05-31 13:49:08.144750: Pseudo dice [0.4612] 
2024-05-31 13:49:08.144917: Epoch time: 193.72 s 
2024-05-31 13:49:08.145009: Yayy! New best EMA pseudo Dice: 0.4205 
2024-05-31 13:49:10.390064:  
2024-05-31 13:49:10.390464: Epoch 40 
2024-05-31 13:49:10.390594: Current learning rate: 0.00117 
2024-05-31 13:52:28.141642: train_loss -0.6156 
2024-05-31 13:52:28.157596: val_loss -0.3788 
2024-05-31 13:52:28.157740: Pseudo dice [0.4226] 
2024-05-31 13:52:28.157884: Epoch time: 197.75 s 
2024-05-31 13:52:28.157971: Yayy! New best EMA pseudo Dice: 0.4207 
2024-05-31 13:52:30.227237:  
2024-05-31 13:52:30.227674: Epoch 41 
2024-05-31 13:52:30.227805: Current learning rate: 0.00107 
2024-05-31 13:56:12.134027: train_loss -0.6096 
2024-05-31 13:56:12.145117: val_loss -0.3489 
2024-05-31 13:56:12.145242: Pseudo dice [0.3934] 
2024-05-31 13:56:12.145366: Epoch time: 221.91 s 
2024-05-31 13:56:13.697886:  
2024-05-31 13:56:13.698069: Epoch 42 
2024-05-31 13:56:13.698188: Current learning rate: 0.00096 
2024-05-31 13:59:43.894187: train_loss -0.6163 
2024-05-31 13:59:43.904512: val_loss -0.3932 
2024-05-31 13:59:43.904665: Pseudo dice [0.4316] 
2024-05-31 13:59:43.905197: Epoch time: 210.2 s 
2024-05-31 13:59:45.470836:  
2024-05-31 13:59:45.471305: Epoch 43 
2024-05-31 13:59:45.471439: Current learning rate: 0.00085 
2024-05-31 14:05:03.204111: train_loss -0.6122 
2024-05-31 14:05:03.215723: val_loss -0.3951 
2024-05-31 14:05:03.215858: Pseudo dice [0.4415] 
2024-05-31 14:05:03.215997: Epoch time: 317.73 s 
2024-05-31 14:05:03.216086: Yayy! New best EMA pseudo Dice: 0.4216 
2024-05-31 14:05:05.262941:  
2024-05-31 14:05:05.263129: Epoch 44 
2024-05-31 14:05:05.263249: Current learning rate: 0.00074 
2024-05-31 14:09:14.072094: train_loss -0.6245 
2024-05-31 14:09:14.078992: val_loss -0.4019 
2024-05-31 14:09:14.079113: Pseudo dice [0.4421] 
2024-05-31 14:09:14.079243: Epoch time: 248.81 s 
2024-05-31 14:09:14.079333: Yayy! New best EMA pseudo Dice: 0.4236 
2024-05-31 14:09:16.080935:  
2024-05-31 14:09:16.081142: Epoch 45 
2024-05-31 14:09:16.081264: Current learning rate: 0.00063 
2024-05-31 14:12:31.485753: train_loss -0.6471 
2024-05-31 14:12:31.497741: val_loss -0.3882 
2024-05-31 14:12:31.497956: Pseudo dice [0.4411] 
2024-05-31 14:12:31.498199: Epoch time: 195.41 s 
2024-05-31 14:12:31.498360: Yayy! New best EMA pseudo Dice: 0.4254 
2024-05-31 14:12:33.515676:  
2024-05-31 14:12:33.516124: Epoch 46 
2024-05-31 14:12:33.516255: Current learning rate: 0.00051 
2024-05-31 14:15:49.007340: train_loss -0.6486 
2024-05-31 14:15:49.018211: val_loss -0.3688 
2024-05-31 14:15:49.018367: Pseudo dice [0.4151] 
2024-05-31 14:15:49.018525: Epoch time: 195.49 s 
2024-05-31 14:15:50.596569:  
2024-05-31 14:15:50.597047: Epoch 47 
2024-05-31 14:15:50.597187: Current learning rate: 0.0004 
2024-05-31 14:19:05.477806: train_loss -0.647 
2024-05-31 14:19:05.487209: val_loss -0.405 
2024-05-31 14:19:05.487335: Pseudo dice [0.4443] 
2024-05-31 14:19:05.487477: Epoch time: 194.88 s 
2024-05-31 14:19:05.487568: Yayy! New best EMA pseudo Dice: 0.4263 
2024-05-31 14:19:07.300717:  
2024-05-31 14:19:07.300933: Epoch 48 
2024-05-31 14:19:07.301061: Current learning rate: 0.00028 
